{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** \n",
    "\n",
    "In this tutorial we will create a simple magnetic problem from scratch using the SimPEG framework.\n",
    "We are using the integral form of the magnetostatic problem. In the absence of free-currents or changing magnetic field, magnetic material can give rise to a secondary magnetic field according to:\n",
    "\n",
    "$$\\vec b = \\frac{\\mu_0}{4\\pi}  \\int_{V}   \\vec M \\cdot \\nabla \\nabla \\left(\\frac{1}{r}\\right) \\; dV $$\n",
    "\n",
    "Where $\\mu_0$ is the magnetic permealitity of free-space, $\\vec M$ is the magnetization per unit volume and $r$  defines the distance between the observed field $\\vec b$ and the magnetized object. Assuming a purely induced response, the strenght of magnetization can be written as:\n",
    "\n",
    "$$ \\vec M = \\mu_0 \\kappa \\vec H_0 $$\n",
    "\n",
    "where $\\vec H$ is an external inducing magnetic field, and $\\kappa$ the magnetic susceptibility of matter.\n",
    "As derived by Sharma 1966, the integral can be evaluated for rectangular prisms such that:\n",
    "\n",
    "$$ \\vec b(P) =  \\mathbf{T} \\cdot \\vec H_0 \\; \\kappa $$\n",
    "\n",
    "Where the tensor matrix $\\bf{T}$ relates the three components of magnetization $\\vec M$ to the components of the field $\\vec b$:\n",
    "\n",
    "$$\\mathbf{T} =\n",
    "\t \\begin{pmatrix}\n",
    "       \t\tT_{xx} & T_{xy} & T_{xz}    \\\\\n",
    "\t\tT_{yx} & T_{yy} & T_{yz}    \\\\\n",
    "\t\tT_{zx} & T_{zy} & T_{zz}           \n",
    "\t\\end{pmatrix} $$\n",
    "    \n",
    "In general, we discretize the earth into a collection of cells, each contributing to the magnetic data such that:\n",
    "\n",
    "$$\\vec b(P) = \\sum_{j=1}^{nc} \\mathbf{T}_j \\cdot \\vec H_0 \\; \\kappa_j$$\n",
    "\n",
    "giving rise to a linear problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Efficiency Warning: Interpolation will be slow, use setup.py!\n",
      "\n",
      "            python setup.py build_ext --inplace\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "## First we need to load all the libraries and set up the paths\n",
    "##\n",
    "## Need to be on \\pf\\dev branch !!!\n",
    "%pylab inline \n",
    "import SimPEG.PF as PF\n",
    "from SimPEG import *\n",
    "from SimPEG.Utils import io_utils\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "import time as tm\n",
    "\n",
    "# Geological surface files and topo surface\n",
    "model_dir = '../../Geological_model/'\n",
    "topofile = model_dir+'TKCtopo.dat'\n",
    "geosurf = [\n",
    "    [model_dir+'Till.ts',True,True,0],\n",
    "    [model_dir+'XVK.ts',True,True,1],\n",
    "    [model_dir+'PK1.ts',True,True,2],\n",
    "    [model_dir+'PK2.ts',True,True,3],\n",
    "    [model_dir+'PK3.ts',True,True,4],\n",
    "    [model_dir+'HK1.ts',True,True,5],\n",
    "    [model_dir+'VK.ts',True,True,6]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We first need to create a mesh and simple survey\n",
    "# Create our own mesh!\n",
    "csx, csy, csz = 20., 20., 20.\n",
    "ncx, ncy, ncz = 25, 25, 20\n",
    "npad = 5\n",
    "hx = [(csx,npad, -1.3),(csx,ncx),(csx,npad, 1.3)]\n",
    "hy = [(csy,npad, -1.3),(csy,ncy),(csy,npad, 1.3)]\n",
    "hz = [(csz,npad, -1.3),(csz,ncz)]\n",
    "mesh = Mesh.TensorMesh([hx, hy, hz],x0=\"CCN\")\n",
    "xc = 300+5.57e5\n",
    "yc = 600+7.133e6\n",
    "zc = 450.\n",
    "x0_new = np.r_[mesh.x0[0]+xc, mesh.x0[1]+yc, mesh.x0[2]+zc]\n",
    "mesh._x0 = x0_new\n",
    "\n",
    "# YOu can write out the mesh if you like\n",
    "#Mesh.TensorMesh.writeUBC(mesh,model_dir+\"PF_mesh_UTM.msh\")\n",
    "\n",
    "# Define no-data-value\n",
    "ndv = -100\n",
    "\n",
    "# Define survey flight height\n",
    "Z_bird = 20.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on a set of parametric surfaces representing TKC, \n",
    "# we use VTK to discretize the 3-D space.\n",
    "# mlab.figure(bgcolor=(1.,1.,1.))\n",
    "modelInd = np.ones(mesh.nC)*ndv\n",
    "for ii in range(len(geosurf)):\n",
    "    tin = tm.time()\n",
    "    print \"Computing indices with VTK: \" + geosurf[ii][0]\n",
    "    T, S = io_utils.read_GOCAD_ts(geosurf[ii][0])\n",
    "    indx = io_utils.surface2inds(T,S,mesh, boundaries=geosurf[ii][1], internal=geosurf[ii][2])\n",
    "    print \"VTK operation completed in \" + str(tm.time() - tin) + \" sec\"\n",
    "    modelInd[indx] = geosurf[ii][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(modelInd), mesh.nC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that we have the surfaces converted to indices, we can build our physical property model\n",
    "def getModel(Till=0.0, XVK=2e-3, PK1=5e-3, PK2=2e-3, PK3=1e-3, HK1=2e-2, VK=1e-2, bkgr=0.):\n",
    "    vals = [Till, XVK, PK1, PK2, PK3, HK1, VK]\n",
    "    model= np.ones(mesh.nC) * bkgr\n",
    "\n",
    "    for ii, sus in zip(range(7),vals):\n",
    "        model[modelInd == ii] = sus\n",
    "    return model\n",
    "model = getModel()\n",
    "sus = getModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load topography file in UBC format and find the active cells\n",
    "topo = np.genfromtxt(topofile,skip_header=1)\n",
    "\n",
    "# Find the active cells\n",
    "actv = Utils.surface2ind_topo(mesh, topo, gridLoc='N')\n",
    "\n",
    "# Create active map to go from reduce set to full\n",
    "actvMap = Maps.InjectActiveCells(mesh, actv, ndv)\n",
    "print \"Active cells created from topography!\"\n",
    "model = model[actv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets.widgets import interact, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here you can visualize the current model\n",
    "m_true = actvMap*model\n",
    "Mesh.TensorMesh.writeModelUBC(mesh,model_dir+\"Synthetic_mag.sus\",m_true)\n",
    "airc = m_true==ndv\n",
    "m_true[airc] = np.nan\n",
    "def slide(s,normal):\n",
    "    colorbar(mesh.plotSlice(m_true, normal=normal, ind=s, clim=np.r_[model.min(), model.max()])[0])\n",
    "    plt.gca().set_aspect('equal')\n",
    "interact(slide, s=(0,60), normal=['X','Y','Z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward system:**\n",
    "\n",
    "Now that we have all our spatial components, we can create our linear system. For a single location and single component of the data, the system would looks like this:\n",
    "\n",
    "$$ b_x =\n",
    "\t\\begin{bmatrix}\n",
    "\tT_{xx}^1 &... &T_{xx}^{nc} & T_{xy}^1 & ... & T_{xy}^{nc} & T_{xz}^1 & ... & T_{xz}^{nc}\\\\\n",
    "\t \\end{bmatrix}\n",
    "\t \\begin{bmatrix}\n",
    "\t\t\\mathbf{M}_x \\\\ \\mathbf{M}_y \\\\ \\mathbf{M}_z\n",
    "\t\\end{bmatrix} \\\\ $$\n",
    "\n",
    "where each of $T_{xx},\\;T_{xy},\\;T_{xz}$ are [nc x 1] long. For the $y$ and $z$ component, we need the two other rows of the tensor $\\mathbf{T}$.\n",
    "In our simple induced case, the magnetization direction $\\mathbf{M_x,\\;M_y\\;,Mz}$ are known and assumed to be constant everywhere, so we can reduce the size of the system such that: \n",
    "\n",
    "$$ \\vec{\\mathbf{d}}_{\\text{pred}} = (\\mathbf{T\\cdot M})\\; \\kappa$$\n",
    "\n",
    "\n",
    "\n",
    "In most geophysical surveys, we are not collecting all three components, but rather the magnitude of the field, or $Total\\;Magnetic\\;Intensity$ (TMI) data.\n",
    "Because the inducing field is really large, we will assume that the anomalous fields are parallel to $H_0$:\n",
    "\n",
    "$$ d^{TMI}  = \\hat H_0 \\cdot \\vec d$$\n",
    "\n",
    "We then end up with a much smaller system:\n",
    "\n",
    "$$ d^{TMI} = \\mathbf{F\\; \\kappa}$$\n",
    "\n",
    "where $\\mathbf{F} \\in \\mathbb{R}^{nd \\times nc}$ is our $forward$ operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import NearestNDInterpolator\n",
    "# We will generate a simple survey with a grid of points drapped over topography\n",
    "# We need to define the direction of the inducing field\n",
    "# From old convention, field orientation is given as an azimuth from North \n",
    "# (positive clockwise) and dip from the horizontal (positive downward).\n",
    "# The field parameters at TKC are [H:60,308 nT, I:83.8 d D:25.4 d ]\n",
    "H0 = (60308.,83.8,25.4)\n",
    "\n",
    "# We create a synthetic survey with observations in cell center.\n",
    "X, Y = np.meshgrid(mesh.vectorCCx[npad:-npad:2], mesh.vectorCCy[npad:-npad:2])\n",
    "\n",
    "# Using our topography, we trape the survey and shift it up by the flight height\n",
    "Ftopo = NearestNDInterpolator(topo[:,:2], topo[:,2])\n",
    "Z = Ftopo(Utils.mkvc(X.T),Utils.mkvc(Y.T)) + Z_bird\n",
    "\n",
    "rxLoc = np.c_[Utils.mkvc(X.T), Utils.mkvc(Y.T), Utils.mkvc(Z.T)]\n",
    "rxLoc = PF.BaseMag.RxObs(rxLoc)\n",
    "\n",
    "srcField = PF.BaseMag.SrcField([rxLoc])\n",
    "srcField.param = H0\n",
    "survey = PF.BaseMag.LinearSurvey(srcField)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that we have a model and a survey we can build the linear system ...\n",
    "nactv = np.int(np.sum(actv))\n",
    "\n",
    "# Creat reduced identity map\n",
    "idenMap = Maps.IdentityMap(nP=nactv)\n",
    "\n",
    "# Create the forward model operator (the argument forwardOnly=False store the forward matrix to memory)\n",
    "prob = PF.Magnetics.Problem3D_Integral(mesh, mapping=idenMap, actInd=actv, forwardOnly=False, rtype = 'tmi')\n",
    "\n",
    "# Pair the survey and problem\n",
    "survey.pair(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fist time that we ask for predicted data,\n",
    "# the dense matrix T is calculated.\n",
    "# This is generally the bottleneck of the integral formulation, \n",
    "# in terms of time and memory.\n",
    "d = prob.fields(model)\n",
    "\n",
    "# Add noise to the data and assign uncertainties\n",
    "data = d + randn(len(d)) # We add some random Gaussian noise (1nT)\n",
    "wd = np.ones(len(data))*1. # Assign flat uncertainties\n",
    "\n",
    "survey.dobs = data\n",
    "survey.std = wd\n",
    "\n",
    "# [OPTIONAL] You can write the observations to UBC format here\n",
    "#PF.Magnetics.writeUBCobs('MAG_Synthetic_data.obs',survey,data)\n",
    "\n",
    "d2D = data.reshape(X.shape)\n",
    "dat = plt.contourf(X-xc,Y-yc, d2D,40)    \n",
    "plt.gca().set_aspect('equal')\n",
    "plt.plot(X.flatten()-xc,Y.flatten()-yc,'k.', ms=2)\n",
    "plt.colorbar(dat)    \n",
    "plt.xlabel(\"Easting (m)\")\n",
    "plt.ylabel(\"Northing (m)\")\n",
    "plt.title(\"Total Magnetic Intensity (nT)\")\n",
    "xlim(-500, 500)\n",
    "ylim(-500, 500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inverse problem**\n",
    "\n",
    "We have generated synthetic data, we now what to see if we can solve the inverse. Using the usual formulation, we seek a model that can reproduce the data, let’s say a least-squares measure of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_d =   \\|\\mathbf{W}_d \\left( \\mathbb{F}[\\mathbf{m}] - \\mathbf{d}^{obs} \\right)\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "The inverse problem is hard because we don’t have great data coverage, and the Earth is big, and there is usually noise in the data. So we need to add something to regularize it.\n",
    "The simplest way to do it is to penalize solutions that won’t make sense geologically, for example to assume that the model is small.\n",
    "The usual smooth inversion function use an l2-norm measure:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_d =   \\|\\mathbf{W}_d \\left( \\mathbb{F}[\\mathbf{m}] - \\mathbf{d}^{obs} \\right)\\|_2^2 \\\\\n",
    "\\phi_m = \\beta \\Big [ {\\| \\mathbf{W}_s \\;( \\mathbf{m - m^{ref}})\\|}^2_2  + \\sum_{i = x,y,z}  {\\|   \\mathbf{W}_i  \\; \\mathbf{G}_i \\; \\mathbf{m}\\|}^2_2  \\Big ]\\;,\n",
    "\\end{equation}\n",
    "\n",
    "The full objective function to be minimized can be written as:\n",
    "\\begin{equation}\n",
    "\\phi(m) =  \\phi_d + \\beta \\phi_m\\;,\n",
    "\\end{equation}\n",
    "which will yield our usual *small* and *smooth* models. \n",
    "\n",
    "We propose a fancier regularization function that can allow to recover *sparse* and *blocky* solutions.\n",
    "Starting with the well known Ekblom norm:\n",
    "\\begin{equation}\n",
    "\\phi_m =  \\sum_{i=1}^{nc} {(x_i^2 + \\epsilon^2)}^{p/2} \\;,\n",
    "\\end{equation}\n",
    "where $x_i$ denotes some function of the model parameter, and $\\epsilon$ is a small value to avoid singularity as $m\\rightarrow0$.\n",
    "For p=2, we get the usual least-squares measure and we recover the regularization presented above. For $p \\leq 1$, the function becomes non-linear which requires some tweaking.\n",
    "\n",
    "We can linearize the function by updating the penality function iteratively, commonly known as an Iterative Re-weighted Least-Squares (IRLS) method:\n",
    "\\begin{equation} \n",
    "\\phi_m^{(k)} =  \\frac{1}{2}\\sum_{i=1}^{nc} r_i \\; x_i^2\n",
    "\\end{equation}\n",
    "where we added the superscript $\\square^{(k)}$ to denote the IRLS iterations. The weights $r(x)$ are computed from model values obtained at a previous iteration such that:\n",
    "\\begin{equation}\n",
    "\t{r}_i  ={\\Big( {({x_i}^{(k-1)})}^{2} + \\epsilon^2 \\Big)}^{p/2 - 1} \\;,\n",
    "\\end{equation}\n",
    "where ${r}(x) \\in \\mathbb{R}^{nc}$.\n",
    "\n",
    "In matrix form, our objective function simply becomes:\n",
    "\\begin{equation}\n",
    "\\phi(m) =   \\|\\mathbf{W}_d \\left( \\mathbb{F}[\\mathbf{m}] - \\mathbf{d}^{obs} \\right)\\|_2^2 + \\beta \\Big [ {\\| \\mathbf{W}_s \\;\\mathbf{R}_s\\;( \\mathbf{m - m^{ref}})\\|}^2_2  + \\sum_{i = x,y,z}  {\\|   \\mathbf{W}_i\\; \\mathbf{R}_i  \\; \\mathbf{G}_i \\; \\mathbf{m}\\|}^2_2  \\Big ]\\;,\n",
    "\\end{equation}\n",
    "where the IRLS weights $\\mathbf{R}_s$ and $\\mathbf{R}_i$ are diagonal matrices defined as:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\t{R}_{s_{jj}}  &=  \\sqrt{\\eta_p}{\\Big[ {({m_j}^{(k-1)})}^{2} + \\epsilon_p^2 \\Big]}^{(p/2 - 1)/2} \\\\\n",
    "\t{R}_{i_{jj}}  &=  \\sqrt{\\eta_q}{\\Big[ {\\left ({{(G_i\\;m^{(k-1)})}_j }\\right)}^{2} + \\epsilon_q^2 \\Big]}^{(q/2 - 1)/2} \\\\\n",
    "\\eta_p &=  {\\epsilon_p}^{(1-p/2)} \\\\\n",
    "\\eta_q &=   {\\epsilon_q}^{(1-q/2)}  \\;, \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "we added two scaling parameters $\\eta_p$ and $\\eta_q$ for reasons that we won't dicuss here, but turn out to be important to get stable solves.\n",
    "\n",
    "In order to initialize the IRLS and get an estimate for the stabilizing parameters $\\epsilon_p$ and $\\epsilon_q$, we first invert with the smooth $l_2$-norm. \n",
    "The whole IRLS process is implemented with a directive added to the inversion workflow (see below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It is potential fields, so we will need to push the inverison down\n",
    "# Create distance weights from our linera forward operator\n",
    "wr = np.sum(prob.G**2.,axis=0)**0.5\n",
    "wr = ( wr/np.max(wr) )\n",
    "#wr = PF.Magnetics.get_dist_wgt(mesh, survey.srcField.rxList[0].locs, actv, 3., np.min(mesh.hx)/4.)\n",
    "#wr = wr**2. # Need to square to be included as weights\n",
    "    \n",
    "reg = Regularization.Sparse(mesh, indActive = actv, mapping = idenMap)\n",
    "reg.cell_weights = wr\n",
    "\n",
    "dmis = DataMisfit.l2_DataMisfit(survey)\n",
    "dmis.Wd = 1/wd\n",
    "\n",
    "# Add directives to the inversion\n",
    "opt = Optimization.ProjectedGNCG(maxIter=100 ,lower=0.,upper=1., maxIterLS = 20, maxIterCG= 10, tolCG = 1e-3)\n",
    "invProb = InvProblem.BaseInvProblem(dmis, reg, opt)\n",
    "betaest = Directives.BetaEstimate_ByEig()\n",
    "\n",
    "# Here is where the norms are applied\n",
    "# Use pick a treshold parameter empirically based on the distribution of model\n",
    "# parameters (run last cell to see the histogram before and after IRLS)\n",
    "eps = [2.5e-4,5e-4] # Threshold parameters to penalize different model parameters\n",
    "norms = [0,1,1,1] # Norms applies on the model and 3 gradients [p, qx, qy, qz]\n",
    "\n",
    "\n",
    "IRLS = Directives.Update_IRLS( norms=norms,  eps=eps, f_min_change = 1e-1, minGNiter=6)\n",
    "update_Jacobi = Directives.Update_lin_PreCond()\n",
    "inv = Inversion.BaseInversion(invProb, directiveList=[IRLS,betaest,update_Jacobi])\n",
    "\n",
    "m0 = np.ones(idenMap.nP)*1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run inversion...\n",
    "mrec = inv.run(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the final model back to full space and plot!!\n",
    "m_lp = actvMap*mrec\n",
    "m_lp[airc] = np.nan\n",
    "\n",
    "# Get the smooth model aslo\n",
    "m_l2 = actvMap*reg.l2model\n",
    "m_l2[airc] = np.nan\n",
    "\n",
    "#[OPTIONAL] Save both models to file\n",
    "#Mesh.TensorMesh.writeModelUBC(mesh,'SimPEG_MAG_l2l2.sus',m_l2)\n",
    "#Mesh.TensorMesh.writeModelUBC(mesh,'SimPEG_MAG_lplq.sus',m_lp)\n",
    "\n",
    "# Plot the recoverd models \n",
    "vmin, vmax = 0., 0.015\n",
    "\n",
    "mesh = Mesh.TensorMesh([mesh.hx, mesh.hy, mesh.hz],x0=\"CCN\")\n",
    "\n",
    "def slide(s,normal):\n",
    "    \n",
    "    if normal == \"Z\":\n",
    "        fig = plt.figure(figsize(10*1.2, 8))\n",
    "    else:\n",
    "        fig = plt.figure(figsize(10*1.2, 4))\n",
    "        \n",
    "    ax1 = plt.subplot(2,2,3)\n",
    "    dat = mesh.plotSlice(m_lp, ax = ax1, normal=normal, ind=s, clim=np.r_[vmin, vmax],pcolorOpts={'cmap':'viridis'})\n",
    "#     plt.colorbar(dat[0])\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.title('Compact model')\n",
    "    \n",
    "    if normal == \"Z\":\n",
    "        xlim(-600, 600)\n",
    "        ylim(-600, 600.)    \n",
    "    else:\n",
    "        xlim(-600, 600)\n",
    "        ylim(-500, 0.) \n",
    "        \n",
    "    ax2 = plt.subplot(2,2,1)\n",
    "    dat = mesh.plotSlice(m_l2, ax = ax2, normal=normal, ind=s, clim=np.r_[vmin, vmax],pcolorOpts={'cmap':'viridis'})\n",
    "#     plt.colorbar(dat[0])\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.title('Smooth model')\n",
    "    \n",
    "    if normal == \"Z\":\n",
    "        xlim(-600, 600)\n",
    "        ylim(-600, 600.)    \n",
    "    else:\n",
    "        xlim(-600, 600)\n",
    "        ylim(-500, 0.) \n",
    "        \n",
    "    ax2.set_xticklabels([])\n",
    "        \n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    dat = mesh.plotSlice(m_true, ax = ax2, normal=normal, ind=s, clim=np.r_[vmin, vmax],pcolorOpts={'cmap':'viridis'})\n",
    "#     plt.colorbar(dat[0])\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.title('True model')\n",
    "    \n",
    "    pos =  ax2.get_position()\n",
    "\n",
    "    ax2.yaxis.set_visible(False)\n",
    "    if normal == \"Z\":\n",
    "        xlim(-600, 600)\n",
    "        ylim(-600, 600.) \n",
    "        ax2.set_position([pos.x0 -0.04 , pos.y0,  pos.width, pos.height])\n",
    "    else:\n",
    "        xlim(-600, 600)\n",
    "        ylim(-500, 0.) \n",
    "\n",
    "    pos =  ax2.get_position()\n",
    "    cbarax = fig.add_axes([pos.x0 + 0.375 , pos.y0 + 0.05,  pos.width*0.1, pos.height*0.75])  ## the parameters are the specified position you set\n",
    "    cb = fig.colorbar(dat[0],cax=cbarax, orientation=\"vertical\", ax = ax2, ticks=np.linspace(vmin,vmax, 4))\n",
    "    cb.set_label(\"Susceptibility (SI)\",size=12)\n",
    "    \n",
    "    #{OPTIONAL} Save the figure to png\n",
    "    #fig.savefig('PF_Compact.png',dpi = 150)\n",
    "    \n",
    "interact(slide, s=(0,34), normal=['Y','Z','X'])\n",
    "\n",
    "# interact(lambda ind: viz(m_l2, ind, normal=\"Z\"), ind=IntSlider(min=0, max=32,step=1, value=28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets compare the distribution of model parameters and model gradients\n",
    "plt.figure(figsize=[15,5])\n",
    "ax = plt.subplot(121)\n",
    "plt.hist(reg.l2model,100)\n",
    "plt.plot((eps[0],eps[0]),(0,mesh.nC),'r--')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.title('Hist model - l2-norm')\n",
    "plt.legend(['$\\epsilon_p$'])\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.hist(reg.regmesh.cellDiffxStencil*reg.l2model,100)\n",
    "plt.plot((eps[1],eps[1]),(0,mesh.nC),'r--')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.title('Hist model gradient - l2-norm')\n",
    "plt.legend(['$\\epsilon_q$'])\n",
    "\n",
    "# Lets look at the distribution of model parameters and model gradients\n",
    "plt.figure(figsize=[15,5])\n",
    "ax = plt.subplot(121)\n",
    "plt.hist(mrec,100)\n",
    "plt.plot((eps[0],eps[0]),(0,mesh.nC),'r--')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.title('Hist model - lp-norm')\n",
    "plt.legend(['$\\epsilon_p$'])\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.hist(reg.regmesh.cellDiffxStencil*mrec,100)\n",
    "plt.plot((eps[1],eps[1]),(0,mesh.nC),'r--')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.title('Hist model gradient - lp-norm')\n",
    "plt.legend(['$\\epsilon_q$'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
